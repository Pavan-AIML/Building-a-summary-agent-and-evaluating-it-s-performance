import openai
import json
import argparse
import tqdm
import time

if __name__ == '__main__':
# creating an argument parser to handle the input parameters.


### -------Instantiating the argument parser ----------------#####

    argparser = argparse.ArgumentParser()

### ------Adding the arguments to the argument parser----------#####


# accessing the prompt that will instruct the model to evaluate the summaries
    argparser.add_argument('--prompt_fp', type=str, default='prompts/summeval/rel_detailed.txt')
# Saving the file with the scores generated by evaluator model
    argparser.add_argument('--save_fp', type=str, default='results/gpt4_summmaries_rel_scores.json')
# The file with the summaries generated by the model that we want to evaluate using the evaluator model 'gpt-4-0613'
    argparser.add_argument('--summeval_fp', type=str, default='results/gpt4_summaries_generatedby_model.json')
# here we will give th eopen AI key as an argument in therminal while running the code. 
    argparser.add_argument('--key', type=str, required=True)
# Selected the gpt evaluator model to evaluate the summaries generated by the model.
    argparser.add_argument('--model', type=str, default='gpt-4-0613')


########---------------------------------Parsing the arguments------------------------------#########
    
    args = argparser.parse_args()


# setting up the openai api key. And we can access the key from the argument parser. by args.key
    openai.api_key = args.key


# reading the json save data file with the summaries generated by the model.
    summeval = json.load(open(args.summeval_fp))


# reading the prompt file that will instruct the model to evaluate the summaries.
    prompt = open(args.prompt_fp).read()

    ct, ignore = 0, 0

# Storing the data with the scores in the respective files after changing th epromts according to coherence, consistancy, relevance and fluency.

    new_json = []
    # use tqdm to check the process.
    for instance in tqdm.tqdm(summeval):
        source = instance['text']
        system_output = instance['model_response']
        cur_prompt = prompt.replace('{{Document}}', source).replace('{{Summary}}', system_output)
        instance['prompt'] = cur_prompt
        while True:
            # reference [https://readmedium.com/model-parameters-in-openai-api-161a5b1f8129?utm_source=chatgpt.com]
            
            try:
                _response = openai.ChatCompletion.create(
                    model=args.model,
                    messages=[{"role": "system", "content": cur_prompt}],
                    temperature=2, # This helps to definte the randomenss of the output. limit -> 0 to 1
                    max_tokens=5, # total tokens to generate as an putput.
                    top_p=1, # 
                    frequency_penalty=0, # This will help to avoid the repetition of the same words.
                    presence_penalty=0, # 
                    stop=None, # This allows the model to generate the text without any stop words. Till it has reached its max tokens or stoping limit.
                    # logprobs=40,
                    n=4
                )
                time.sleep(0.5)
# the loop is through all th e4 choices as the model is generating the 4 choices for each of the input.
                all_responses = [_response['choices'][i]['message']['content'] for i in
                                 range(len(_response['choices']))]
                instance['all_responses'] = all_responses
                new_json.append(instance)
                ct += 1
                break
# Error handeling if any occurs in try block.
            except Exception as e:
                print(e)
                if ("limit" in str(e)):
                    time.sleep(2)
                else:
                    ignore += 1
                    print('ignored', ignore)

                    break
# For debugging or monitoring purposes. adding the below code. 

    print('ignored total', ignore)
    with open(args.save_fp, 'w') as f:
        json.dump(new_json, f, indent=4)
