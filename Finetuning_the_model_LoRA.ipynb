{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import peft\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Providing the path of the input file \n",
    "\n",
    "input_file_train  = 'data/billsum_train.json'  # Replace with your JSON file path\n",
    "output_file_train = 'data/output_train.jsonl'  # Path to save the JSONL file\n",
    "\n",
    "# Reading the input data file \n",
    "with open(input_file_train, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Write the data to a JSONL file\n",
    "with open(output_file_train, 'w', encoding='utf-8') as f:\n",
    "    for record in data:\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "        f.write('\\n')  # Write a newline after each record to separate the JSON objects\n",
    "\n",
    "input_file_val  = 'data/billsum_val.json'  # Replace with your JSON file path\n",
    "output_file_val = 'data/output_val.jsonl'  # Path to save the JSONL file\n",
    "\n",
    "# Reading the input data file \n",
    "with open(input_file_val, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Write the data to a JSONL file\n",
    "with open(output_file_val, 'w', encoding='utf-8') as f:\n",
    "    for record in data:\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "        f.write('\\n')  # Write a newline after each record to separate the JSON objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = [json.loads(line.strip()) for line in file]\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "file_path = 'data/output.jsonl'  # Replace with your actual file path\n",
    "data = pd.read_json(\"data/output_train.jsonl\", lines=True)\n",
    "dev_df = pd.read_json(\"data/output_val.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The people of the State of California do enact...</td>\n",
       "      <td>(1) Existing law regulates pawnbrokers and req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The people of the State of California do enact...</td>\n",
       "      <td>Existing property tax law establishes a vetera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The people of the State of California do enact...</td>\n",
       "      <td>Existing law, the Federal Surplus Property Acq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The people of the State of California do enact...   \n",
       "1  The people of the State of California do enact...   \n",
       "2  The people of the State of California do enact...   \n",
       "\n",
       "                                             summary  \n",
       "0  (1) Existing law regulates pawnbrokers and req...  \n",
       "1  Existing property tax law establishes a vetera...  \n",
       "2  Existing law, the Federal Surplus Property Acq...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Selecting the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I have selected the available model fron chatgpt right now from transformers pipeline. As the used model is not available from the transformers library. But the optimizing process will be same. Need to check the target modules.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenoze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        inputs = tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        targets = tokenizer(\n",
    "            examples['summary'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Ensure that targets are converted to lists of integers\n",
    "        inputs['labels'] = targets['input_ids'].tolist()\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    # Create a Dataset from the DataFrame\n",
    "    dataset = Dataset.from_pandas(df[['text', 'summary']])\n",
    "    return dataset.map(tokenize_function, batched=True, remove_columns=['text', 'summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20dcf362934846a5a49b042a553685eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8080a45bf8aa4c77a3844012e9832a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the padding token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "\n",
    "# Add pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "train_dataset = preprocess_data(data, tokenizer)\n",
    "# test_dataset = preprocess_data(test_df, tokenizer)\n",
    "dev_dataset = preprocess_data(dev_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rank of the low rank adaptation(r) = 8\n",
    "- Scaling factor lora_alpha = 16 \n",
    "- lora_dropout = 0.1\n",
    "- target_modules = [\"q\", \"k\", \"v\", \"o\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                      # Rank of the low-rank adaptation (higher = more capacity)\n",
    "    lora_alpha=16,           # Scaling factor for updates (higher = larger updates)\n",
    "    lora_dropout=0.1,        # Dropout rate for LoRA layers (higher = more regularization)\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\"]  # Layers to apply LoRA (more layers = more complexity)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class LossLogger(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Called during logging events, logs contain loss and other metrics.\"\"\"\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "\n",
    "    def save_losses(self, output_dir):\n",
    "        \"\"\"Save the logged losses to a file.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        file_path = os.path.join(output_dir, \"losses.txt\")\n",
    "        with open(file_path, \"w\") as f:\n",
    "            for loss in self.losses:\n",
    "                f.write(f\"{loss}\\n\")\n",
    "        print(f\"Losses saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_logger = LossLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/cpzzpnh51yvfh6ctzlsm0jsw0000gn/T/ipykernel_67206/2886628021.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Define a writable directory for model outputs and logs\n",
    "output_dir = './t5large-finetuned-lora'  # Ensure this is writable in your current working directory\n",
    "logging_dir = './logs'  # Directory for storing logs, ensure it's writable\n",
    "\n",
    "# Create the directories if they do not exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "# Define the training settings\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,            # Directory for saving the trained model\n",
    "    per_device_train_batch_size=8,    # Batch size for training\n",
    "    per_device_eval_batch_size=8,     # Batch size for evaluation\n",
    "    num_train_epochs=1,               # Number of training cycles\n",
    "    logging_dir=logging_dir,          # Directory for logs\n",
    "    logging_steps=25,                 # Frequency of logging\n",
    "    evaluation_strategy=\"epoch\",      # Evaluate at the end of each epoch\n",
    "    eval_steps=25,                    # Frequency of evaluations\n",
    "    save_strategy=\"epoch\",            # Save the model after each epoch\n",
    "    learning_rate=1e-4,               # Learning rate\n",
    "    weight_decay=0.01,                # Weight decay\n",
    "    remove_unused_columns=False       # Keep all dataset columns for debugging\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with all the configurations and datasets\n",
    "trainer = Trainer(\n",
    "    model=model,                              # The model to train\n",
    "    args=training_args,                       # Training settings\n",
    "    train_dataset=train_dataset,              # Dataset for training\n",
    "    eval_dataset=dev_dataset,                 # Dataset for evaluation\n",
    "    tokenizer=tokenizer,                      # Tokenizer for text processing\n",
    "    callbacks=[loss_logger]                   # Attach the loss logger\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Save the losses to CSV files\n",
    "loss_logger.save_losses('/logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model saving in the local file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pavankumar/Documents/Winter_Semester24/Applications/AI_startup/Actual_work/Getting_the_summaries_of_each_response_from_model'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/finetuned model'\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Train a model for predictive maintenance\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=50)\n",
    "\n",
    "# Decode the generated tokens\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictive_maintenance_model(X='maintenance_data', y='predictive_labels')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
