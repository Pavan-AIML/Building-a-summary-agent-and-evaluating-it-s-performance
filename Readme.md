<h1> In this project our task is to build an autnomous system that validates the outputs of an AI agent that creates summary for a supply-chain company and Improves it by the time </h1>

<h3> Shortcomings of the agents </h3>

- It may hallucinate insights that are not supported by the data.
- It sometimes produces irrelevant or incoherent summaries.
- Logical inconsistencies may occur between the feedback and the generated output.

<h1> Solution Strategy </h1>

- To monitor the performance of the AI agent, we will use a validation system that checks the output of the agent against the data and provides feedback. The feedback will be used to improve the agent's performance over time.

<h2> Classical techniques to monitor the agent's output</h2>

- The evaluation has been done in the file gpt4-eval_evaluator.py and the results as per all the catagories have been stored in the file "results/...scores.json". And model API has been invoked 5 times for the same task. 


- Evaluate using ROUGE -: ROUGE is a set of metrics used for evaluating automatic summarization and machine translation software in natural language processing. 

- Evaluate using BERTScore -: BERTScore measures the similarity between two texts based on the contextu-alized embedding from BERT

<h2> Advanced technique to monitor the agent's output</h2>


- Evaluate using GPT-4 called as G-Eval this is a prompt-based evaluator with three main components:
Here we will calculate the average accuracy and check weather model is performing well or not.

1> A prompt that contains the defenition of the evaluation task and the desired evaluation criteria.
2> A chain of thoughts (CoT) these are the set of intermediate instructions generated by the LLM describing the detailed evaluation steps.
3> Scoring a function that calls LLM and calculates the probabilities of the return tokens. 

<h3> Advantages of this technique </h3>

- For the classical techniques we need the reference summaries to do the comparison and that is a huge task in itself. But in the case of GPT-4 we don't need the reference summaries to do the comparison. Here we ned the input prompt (Reviews of the customers) and the output of the agent (Summary of the reviews) and we can do the comparison.

<h4> Overview of this method here we have four distinct criteria to evaluate the output of the agent </h4>

<img src="evaluetor's_performance.png" alt="Alt text" width="300" height="200">

- Relevance (1-5): As summry should be consice and precise hence relevance will check if the summary is relevant to the input prompt and should not include redundancies. 

- Coherence (1-5): The collective quality of all sentences. The summary should be well structured and organized. Summary should be well organized it just not be some sentences related to the input prompt.

- Consistency (1-5): Summary should based on the facts of the input prompt. It should not include any hallucinations. It will read the aummary and the input prompt and check if the summary is consistent with the input prompt.


- Fluency (1-3): It is the quality of the summary in terms of grammer, punctuation, spelling and word choices.


In the below list of scores we can check the comparison of this method and others. 


<img src="setting_up_evaluator/images/evaluator's_performance.png" alt="Alt text" width="300" height="200">


G-Eval-4 is better in terms of all the Metrices Coherence, Consistency, Fluency and Relevance.

Comparision between the Human summary and GPT summary.

<img src="setting_up_evaluator/images/Results_of_summaries.png" alt="Alt text" width="300" height="200">



[Referred dataset >>>>>](https://huggingface.co/docs/transformers/en/tasks/summarization)

Generating the summaries from the dataset text feeding to the LLM and getting the response as per the 

And after this calculating the average score of the model if the evaluator is evaluating higer in the genearted summaries in comparision to the human summaries or becnh mark summeries then we are good to go. If not then we can try finetuning the model from different ways.





<h2> Stratigies to optimize improve the AI agent</h2>

<h3> Model selection on the task specific </h3

by using the available dash boards such as  [scale dahs-boards](https://scale.com/leaderboards) we can select the model which is best suited for the task.


<h3> Data </h3>

> Storing and cleaning the data.

> digging in ot the data to check if the data is formatted according to our available computer power if not reduce the data size or increase the computer power.


> Investigate the chunking strategy in RAG so that right content has been provided. Increasre the number of chunks in the retriever Optimizing the context retrieval in the Lang chain. Here I have provided 3 but we can provide more. 

> We can use fewshots prompting also to improve the model's accuracy.

> Providing a solid prompt to stop model from hallucinating.


<h3> Fine-tuning the model to improve accuracy </h3>


> To improve the model's accuracy we can fine-tune the model with QLORA technique. 
> I have done done the fine tuning of the model in the file name Finetuning_the_model_LoRA.ipynb.
> This technique is used to fine-tune the model with less computational power.
> This I tried doing but the my PC does not have that much capacity to do it. 

> Your input is highly appreciated. 